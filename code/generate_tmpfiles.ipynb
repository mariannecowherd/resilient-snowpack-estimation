{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62eda261-5604-4602-a26b-9e900829449e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/c/cowherd/.conda/envs/fos/lib/python3.11/site-packages/pyproj/__init__.py:89: UserWarning: pyproj unable to set database path.\n",
      "  _pyproj_global_context_initialize()\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import glob \n",
    "import gc\n",
    "import dask\n",
    "\n",
    "##\n",
    "## set the directories in which your copies of the data are in the dirs.py file\n",
    "from dirs import wrfdir, basedir, coorddir, snoteldir\n",
    "from myutils import wrfread_gcm, metaread, read_merge, relativize_snow_values\n",
    "savedir = f'{basedir}/data/tmp/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879eeb2c-535b-4ffd-b743-54443e55de64",
   "metadata": {},
   "source": [
    "we want to create some data files that contain pre-computed values from the downcsaled GCM outputs you can save these in a tmp folder while working on the project -- they are much smaller than the full  dataset but are directly repetitive\n",
    "\n",
    "1. snowMax.nc \n",
    "2. allcorrs_huc2.npy\n",
    "3. snotel_from_WRF_BC.nc\n",
    "4. all_snotelWRF_365.nc\n",
    "5. plots_data.pkl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ace48b87-8f06-4dbf-8fdd-2150cf69f268",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 9 9 9\n",
      "365_day 2014-08-31 2100-08-31\n",
      "cesm2 r11i1p1f1 True ssp370\n",
      "proleptic_gregorian 2014-08-31 2100-08-31\n",
      "mpi-esm1-2-lr r7i1p1f1 True ssp370\n",
      "proleptic_gregorian 2014-08-31 2100-08-31\n",
      "cnrm-esm2-1 r1i1p1f2 True ssp370\n",
      "proleptic_gregorian 2014-08-31 2100-08-31\n",
      "ec-earth3-veg r1i1p1f1 True ssp370\n",
      "365_day 2014-08-31 2100-08-31\n",
      "fgoals-g3 r1i1p1f1 True ssp370\n",
      "360_day 2014-08-30 2100-08-30\n",
      "ukesm1-0-ll r2i1p1f2 True ssp370\n",
      "365_day 2014-08-31 2100-08-31\n",
      "canesm5 r1i1p2f1 True ssp370\n",
      "proleptic_gregorian 2014-08-31 2100-08-31\n",
      "access-cm2 r5i1p1f1 True ssp370\n",
      "proleptic_gregorian 2014-08-31 2100-08-31\n",
      "ec-earth3 r1i1p1f1 True ssp370\n",
      "CPU times: user 1min 59s, sys: 5min 1s, total: 7min\n",
      "Wall time: 4min 31s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9738"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## based on code from Stefan Rahimi\n",
    "date_start_hist, date_end_hist = \"1980-09-01\", \"2014-08-31\"\n",
    "date_start_ssp, date_end_ssp = \"2014-09-01\", \"2100-08-31\"\n",
    "\n",
    "domain = \"d02\"\n",
    "lat1, lon1, z1, file =  metaread(coorddir,domain)\n",
    "coords2 = xr.open_dataset(file)\n",
    "lon_wrf = lon1[0,:,:]\n",
    "lat_wrf = lat1[0,:,:]\n",
    "z_wrf = z1[0,:,:]\n",
    "lat_wrf = xr.DataArray(lat_wrf, dims=[\"lat2d\", \"lon2d\"])\n",
    "lon_wrf = xr.DataArray(lon_wrf, dims=[\"lat2d\", \"lon2d\"])\n",
    "z_wrf = xr.DataArray(z_wrf, dims=[\"lat2d\", \"lon2d\"])\n",
    "\n",
    "coords = {'lat': (['lat2d','lon2d'], lat_wrf), \\\n",
    "                 'lon': (['lat2d','lon2d'], lon_wrf) }\n",
    "\n",
    "gcms = ['cesm2','mpi-esm1-2-lr','cnrm-esm2-1',\n",
    "        'ec-earth3-veg','fgoals-g3','ukesm1-0-ll',\n",
    "        'canesm5','access-cm2','ec-earth3']\n",
    "\n",
    "variants = ['r11i1p1f1','r7i1p1f1','r1i1p1f2',\n",
    "            'r1i1p1f1','r1i1p1f1','r2i1p1f2',\n",
    "            'r1i1p2f1','r5i1p1f1','r1i1p1f1',\n",
    "            ]\n",
    "\n",
    "calendar = ['365_day','proleptic_gregorian','proleptic_gregorian',\n",
    "            'proleptic_gregorian','365_day','360_day',\n",
    "             '365_day','proleptic_gregorian','proleptic_gregorian',\n",
    "            ]\n",
    "\n",
    "ssps = ['ssp370','ssp370','ssp370','ssp370',\n",
    "        'ssp370','ssp370','ssp370','ssp370',\n",
    "        'ssp370','ssp370','ssp370','ssp370',\n",
    "        ]\n",
    "\n",
    "print (len(ssps),len(calendar),len(variants),len(gcms))\n",
    "\n",
    "\n",
    "snow_BC = {}\n",
    "snowMax = []\n",
    "\n",
    "keys = []\n",
    "\n",
    "for count, igcm in enumerate(gcms):\n",
    "    \n",
    "    condition = ()\n",
    "    \n",
    "    bc = True\n",
    "    var = 'snow'\n",
    "    key = '%s_%s_%s' %(igcm,variants[count],ssps[count])\n",
    "    var = read_merge(wrfdir,domain,var,\n",
    "                                        igcm,variants[count], \\\n",
    "                                        date_start_hist,date_start_ssp, \\\n",
    "                                        date_end_hist,date_end_ssp,ssps[count],bc,\n",
    "            calendar[count])\n",
    "    \n",
    "    snow_BC[key] = var\n",
    "    \n",
    "    var_x = var.groupby(var.time.dt.year).max()\n",
    "    snowMax.append(var_x)\n",
    "\n",
    "    \n",
    "snowMax = xr.concat(snowMax,dim='gcm')\n",
    "snowMax.to_netcdf(f'{savedir}snowMax.nc')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6eaae5-d127-4c7d-8c42-cc3f4e5e5338",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PREP SNOTEL WRF DATA ##\n",
    "jjj = x_indices\n",
    "iii = y_indices\n",
    "snow_SNOTELfromWRF= []\n",
    "for icount, igcm in enumerate(snow_BC.keys()):\n",
    "    snow_x = snow_BC[igcm].isel(lat2d=xr.DataArray(jjj,dims=\"site\"), lon2d=xr.DataArray(iii,dims=\"site\"))\n",
    "    start_date = snow_x.time[5*365]  # 1985\n",
    "    end_date = snow_x.time[35*365]   # 2015\n",
    "\n",
    "    snow = snow_x.sel(time = slice(start_date,end_date))\n",
    "    \n",
    "    if icount == 0:\n",
    "        times = snow.time\n",
    "        \n",
    "    if icount != 0:\n",
    "        snow['time'] = times\n",
    "        \n",
    "    snow_SNOTELfromWRF.append(snow)\n",
    "    print (igcm)\n",
    "    \n",
    "snow_SNOTELfromWRF_BC = xr.concat(snow_SNOTELfromWRF,dim='gcm')\n",
    "snow_SNOTELfromWRF_BC['gcm'] = keys\n",
    "\n",
    "zSNOTEL = z_wrf.isel(lat2d=xr.DataArray(jjj,dims=\"site\"), lon2d=xr.DataArray(iii,dims=\"site\")).load()\n",
    "z_coords = {'zWRF': (['site'],  zSNOTEL)}\n",
    "\n",
    "print (snow_SNOTELfromWRF_BC)\n",
    "\n",
    "gc.collect()\n",
    "snotel_from_WRF_BC.to_netcdf('snotel_from_WRF_BC.nc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea45523e-20b1-4508-85df-2678dabd54ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## this takes 50 minutes\n",
    "huc2masks = xr.open_dataset(f'{outputsdir}basin_masks_filtered.nc')\n",
    "\n",
    "all_SDVS = []\n",
    "allcorrs = {}\n",
    "for basinnum in range(8):\n",
    "    mask_bool = huc2masks['basin_mask'][basinnum].values.astype(bool)\n",
    "    swe_local_dataset = sweMax.where(mask_bool)\n",
    "    SDV_all_max = relativize_snow_values(swe_local_dataset)\n",
    "    all_SDVS.append(SDV_all_max)\n",
    "    maxcorr = []\n",
    "    #\n",
    "    allcorrs[basinnum] = {}\n",
    "    #\n",
    "    for gcm in range(9):\n",
    "        #\n",
    "        allcorrs[basinnum][gcm] = {}\n",
    "        #\n",
    "        for year in range(20,121):\n",
    "            #\n",
    "            allcorrs[basinnum][gcm][year] = []\n",
    "            #\n",
    "            corrs = []\n",
    "            for backyear in range(year):\n",
    "                try:\n",
    "                    corr = xr.corr(SDV_all_max['relativized_swe'][gcm,year], SDV_all_max['relativized_swe'][gcm,backyear], dim=['lat2d', 'lon2d'])\n",
    "                    corrs.append(corr.values.flatten()[0])\n",
    "                except RuntimeWarning:\n",
    "                    corrs.append(np.nan)\n",
    "            allcorrs[basinnum][gcm][year] = corrs\n",
    "        gc.collect()\n",
    "    print(basinnum)\n",
    "np.save(f'{basedir}/data/allcorrs_huc2', allcorrs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218a0a2d-7cd2-4596-8a0a-5bb11bee1bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## all_snotelWRF_365.nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ce5354-19b4-4bb4-8c28-cf9be382887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## plots_data.pkl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fos",
   "language": "python",
   "name": "fos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
